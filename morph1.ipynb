{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19381387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üé≠ G√âN√âRATION DE MORPHINGS SUR TOUTE LA BASE DE DONN√âES\n",
      "======================================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   - Mode: all\n",
      "   - Alpha values: [0.5]\n",
      "   - Taille des images: 128x128\n",
      "   - Dossier de sortie: morphing_results\n",
      "   - Sauvegarder individuellement: True\n",
      "   - Cr√©er grille: True\n",
      "\n",
      "‚úì Mod√®le Dlib d√©j√† pr√©sent\n",
      "‚úì Mod√®le Dlib charg√©\n",
      "\n",
      "üì• Chargement du dataset LFW...\n",
      "‚úÖ Dataset charg√©!\n",
      "   - Total images: 2370\n",
      "   - Personnes: 34\n",
      "   - Taille: 62x47\n",
      "\n",
      "======================================================================\n",
      "‚öôÔ∏è  PARAM√àTRES\n",
      "======================================================================\n",
      "\n",
      "Voulez-vous modifier les param√®tres? (d√©faut: non)\n",
      "Configuration actuelle:\n",
      "  - Mode: all\n",
      "  - Alpha values: [0.5]\n",
      "\n",
      "Mode de g√©n√©ration:\n",
      "  1. all - TOUTES les combinaisons (peut √™tre tr√®s long!)\n",
      "  2. sample - Un √©chantillon al√©atoire\n",
      "  3. per_person - Un morphing par personne\n",
      "\n",
      "üî¢ G√©n√©ration de TOUS les morphings\n",
      "   - Nombre de personnes: 34\n",
      "   - Combinaisons possibles: 561\n",
      "   - Alpha values: 1\n",
      "   - Total morphings √† g√©n√©rer: 561\n",
      "\n",
      "üöÄ D√©but de la g√©n√©ration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Morphing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 561/561 [00:50<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ G√©n√©ration termin√©e!\n",
      "   - Temps √©coul√©: 51.0 secondes (0.8 minutes)\n",
      "   - Images g√©n√©r√©es: 561\n",
      "   - Vitesse: 11.01 images/seconde\n",
      "\n",
      "üìä Cr√©ation de la grille...\n",
      "   ‚úì Grille sauvegard√©e: morphing_results\\morphing_grid_5x5.png\n",
      "\n",
      "üíæ Tous les fichiers sont dans: morphing_results\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TERMIN√â!\n",
      "======================================================================\n",
      "\n",
      "üìÇ Vos morphings sont dans: morphing_results\n",
      "\n",
      "üí° Astuce: Utilisez MODE='sample' pour des tests rapides!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script pour g√©n√©rer des morphings sur TOUTE la base de donn√©es LFW\n",
    "Cr√©e des morphings entre toutes les paires de personnes possibles\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import bz2\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ------------------ Configuration ------------------\n",
    "LOCAL_DATA_DIR = Path(\"./dlib_models\")\n",
    "LOCAL_DATA_DIR.mkdir(exist_ok=True)\n",
    "PREDICTOR_PATH = LOCAL_DATA_DIR / \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "# Configuration du dataset\n",
    "MIN_FACES_PER_PERSON = 30  # Minimum d'images par personne\n",
    "RESIZE_FACTOR = 0.5        # Facteur de redimensionnement\n",
    "SIZE = 128                 # Taille finale des images\n",
    "\n",
    "# Configuration de g√©n√©ration\n",
    "OUTPUT_DIR = Path(\"./morphing_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Options de g√©n√©ration\n",
    "MODE = \"all\"  # \"all\" = toutes les combinaisons, \"sample\" = √©chantillon, \"per_person\" = 1 morphing par personne\n",
    "NUM_SAMPLES = 100  # Si MODE=\"sample\", nombre de morphings √† g√©n√©rer\n",
    "ALPHA_VALUES = [0.5]  # Liste des alphas √† tester (ex: [0.3, 0.5, 0.7])\n",
    "SAVE_INDIVIDUAL = True  # Sauvegarder chaque morphing individuellement\n",
    "CREATE_GRID = True  # Cr√©er une grille de morphings\n",
    "GRID_SIZE = (5, 5)  # Taille de la grille (lignes, colonnes)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üé≠ G√âN√âRATION DE MORPHINGS SUR TOUTE LA BASE DE DONN√âES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   - Mode: {MODE}\")\n",
    "if MODE == \"sample\":\n",
    "    print(f\"   - Nombre d'√©chantillons: {NUM_SAMPLES}\")\n",
    "print(f\"   - Alpha values: {ALPHA_VALUES}\")\n",
    "print(f\"   - Taille des images: {SIZE}x{SIZE}\")\n",
    "print(f\"   - Dossier de sortie: {OUTPUT_DIR}\")\n",
    "print(f\"   - Sauvegarder individuellement: {SAVE_INDIVIDUAL}\")\n",
    "print(f\"   - Cr√©er grille: {CREATE_GRID}\")\n",
    "\n",
    "# ------------------ T√©l√©charger le mod√®le Dlib ------------------\n",
    "def download_dlib_predictor():\n",
    "    \"\"\"T√©l√©charge le fichier shape_predictor_68_face_landmarks.dat si n√©cessaire\"\"\"\n",
    "    if PREDICTOR_PATH.exists():\n",
    "        print(f\"\\n‚úì Mod√®le Dlib d√©j√† pr√©sent\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüì• T√©l√©chargement du mod√®le Dlib...\")\n",
    "    url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "    compressed_file = LOCAL_DATA_DIR / \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, compressed_file)\n",
    "        print(\"   D√©compression...\")\n",
    "        \n",
    "        with bz2.BZ2File(compressed_file, 'rb') as f_in:\n",
    "            with open(PREDICTOR_PATH, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        \n",
    "        compressed_file.unlink()\n",
    "        print(\"   ‚úì Mod√®le Dlib pr√™t\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå Erreur lors du t√©l√©chargement du mod√®le Dlib : {e}\")\n",
    "\n",
    "# ------------------ Charger LFW Dataset ------------------\n",
    "def load_lfw_compact():\n",
    "    \"\"\"Charge le dataset LFW\"\"\"\n",
    "    print(\"\\nüì• Chargement du dataset LFW...\")\n",
    "    \n",
    "    try:\n",
    "        lfw_people = fetch_lfw_people(\n",
    "            min_faces_per_person=MIN_FACES_PER_PERSON,\n",
    "            resize=RESIZE_FACTOR,\n",
    "            color=False\n",
    "        )\n",
    "        \n",
    "        images = lfw_people.images\n",
    "        labels = lfw_people.target\n",
    "        target_names = lfw_people.target_names\n",
    "        \n",
    "        n_samples, h, w = images.shape\n",
    "        n_classes = len(target_names)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset charg√©!\")\n",
    "        print(f\"   - Total images: {n_samples}\")\n",
    "        print(f\"   - Personnes: {n_classes}\")\n",
    "        print(f\"   - Taille: {h}x{w}\")\n",
    "        \n",
    "        return images, labels, target_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# T√©l√©charger le mod√®le Dlib\n",
    "download_dlib_predictor()\n",
    "\n",
    "# Charger Dlib\n",
    "try:\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(str(PREDICTOR_PATH))\n",
    "    print(\"‚úì Mod√®le Dlib charg√©\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"‚ùå Impossible de charger Dlib: {e}\")\n",
    "\n",
    "# Charger le dataset\n",
    "images, labels, target_names = load_lfw_compact()\n",
    "\n",
    "# ------------------ Fonctions de morphing ------------------\n",
    "\n",
    "def get_landmarks(img_gray, detector, predictor, upsample_times=0):\n",
    "    \"\"\"D√©tecte les landmarks faciaux\"\"\"\n",
    "    dets = detector(img_gray, upsample_times)\n",
    "    if len(dets) == 0:\n",
    "        return None\n",
    "    shape = predictor(img_gray, dets[0])\n",
    "    pts = np.zeros((68, 2), dtype=np.int32)\n",
    "    for i in range(68):\n",
    "        pts[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return pts\n",
    "\n",
    "def add_corner_points(points, w, h):\n",
    "    corners = np.array([\n",
    "        [0, 0], [w - 1, 0], [w - 1, h - 1], [0, h - 1],\n",
    "        [w // 2, 0], [w - 1, h // 2], [w // 2, h - 1], [0, h // 2]\n",
    "    ], dtype=np.int32)\n",
    "    return np.concatenate([points, corners], axis=0)\n",
    "\n",
    "def clamp_points(points, w, h):\n",
    "    pts = np.array(points, dtype=np.float32)\n",
    "    pts[:, 0] = np.clip(pts[:, 0], 0, w - 1)\n",
    "    pts[:, 1] = np.clip(pts[:, 1], 0, h - 1)\n",
    "    return pts\n",
    "\n",
    "def find_point_index(points, pt, tol=3.0):\n",
    "    pts = np.asarray(points, dtype=np.float32)\n",
    "    dists = np.linalg.norm(pts - np.asarray(pt, dtype=np.float32), axis=1)\n",
    "    idx = int(np.argmin(dists))\n",
    "    if dists[idx] <= tol:\n",
    "        return idx\n",
    "    return None\n",
    "\n",
    "def triangle_completely_inside(t, w, h):\n",
    "    for (x, y) in t:\n",
    "        if x < 0 or x >= w or y < 0 or y >= h:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def apply_affine_transform(src, src_tri, dst_tri, size):\n",
    "    warp_mat = cv2.getAffineTransform(np.float32(src_tri), np.float32(dst_tri))\n",
    "    dst = cv2.warpAffine(src, warp_mat, (int(size[0]), int(size[1])),\n",
    "                         None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "    return dst\n",
    "\n",
    "def morph_triangle(img1, img2, img_morphed, t1, t2, t_morphed, alpha):\n",
    "    r1 = cv2.boundingRect(np.float32([t1]))\n",
    "    r2 = cv2.boundingRect(np.float32([t2]))\n",
    "    r = cv2.boundingRect(np.float32([t_morphed]))\n",
    "\n",
    "    if r1[2] <= 0 or r1[3] <= 0 or r2[2] <= 0 or r2[3] <= 0 or r[2] <= 0 or r[3] <= 0:\n",
    "        return\n",
    "\n",
    "    t1_rect = [(t1[i][0] - r1[0], t1[i][1] - r1[1]) for i in range(3)]\n",
    "    t2_rect = [(t2[i][0] - r2[0], t2[i][1] - r2[1]) for i in range(3)]\n",
    "    t_rect = [(t_morphed[i][0] - r[0], t_morphed[i][1] - r[1]) for i in range(3)]\n",
    "\n",
    "    img1_rect = img1[r1[1]:r1[1]+r1[3], r1[0]:r1[0]+r1[2]]\n",
    "    img2_rect = img2[r2[1]:r2[1]+r2[3], r2[0]:r2[0]+r2[2]]\n",
    "\n",
    "    if img1_rect.size == 0 or img2_rect.size == 0:\n",
    "        return\n",
    "\n",
    "    size_rect = (r[2], r[3])\n",
    "\n",
    "    warp_img1 = apply_affine_transform(img1_rect, t1_rect, t_rect, size_rect)\n",
    "    warp_img2 = apply_affine_transform(img2_rect, t2_rect, t_rect, size_rect)\n",
    "\n",
    "    img_rect = (1.0 - alpha) * warp_img1 + alpha * warp_img2\n",
    "\n",
    "    mask = np.zeros((r[3], r[2]), dtype=np.float32)\n",
    "    cv2.fillConvexPoly(mask, np.int32(t_rect), 1.0, 16, 0)\n",
    "\n",
    "    y, x, w_rect, h_rect = r[1], r[0], r[2], r[3]\n",
    "    img_morphed[y:y+h_rect, x:x+w_rect] = img_morphed[y:y+h_rect, x:x+w_rect] * (1 - mask[:, :, None]) + img_rect * mask[:, :, None]\n",
    "\n",
    "def prepare_points_for_image(img_gray, detector, predictor, w, h):\n",
    "    pts = get_landmarks(img_gray, detector, predictor, upsample_times=0)\n",
    "    if pts is None:\n",
    "        grid_x = np.tile(np.linspace(w*0.25, w*0.75, 17), (4,))\n",
    "        grid_y = np.repeat(np.linspace(h*0.25, h*0.75, 4), 17)\n",
    "        grid = np.vstack([grid_x[:68], grid_y[:68]]).T.astype(np.int32)\n",
    "        pts = grid\n",
    "    pts = clamp_points(pts, w, h)\n",
    "    pts = add_corner_points(pts.astype(np.int32), w, h)\n",
    "    return pts.astype(np.float32)\n",
    "\n",
    "def morph_faces(imgA, imgB, alpha=0.5):\n",
    "    \"\"\"Morphe deux visages\"\"\"\n",
    "    # Redimensionner\n",
    "    imgA_resized = cv2.resize(imgA, (SIZE, SIZE), interpolation=cv2.INTER_CUBIC)\n",
    "    imgB_resized = cv2.resize(imgB, (SIZE, SIZE), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Convertir en uint8\n",
    "    if imgA_resized.dtype != np.uint8:\n",
    "        imgA_resized = (imgA_resized * 255).astype(np.uint8)\n",
    "    if imgB_resized.dtype != np.uint8:\n",
    "        imgB_resized = (imgB_resized * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convertir en couleur\n",
    "    imgA_color = cv2.cvtColor(imgA_resized, cv2.COLOR_GRAY2BGR).astype(np.float32)\n",
    "    imgB_color = cv2.cvtColor(imgB_resized, cv2.COLOR_GRAY2BGR).astype(np.float32)\n",
    "\n",
    "    # Pr√©parer les points\n",
    "    ptsA = prepare_points_for_image(imgA_resized, detector, predictor, SIZE, SIZE)\n",
    "    ptsB = prepare_points_for_image(imgB_resized, detector, predictor, SIZE, SIZE)\n",
    "\n",
    "    # Points morph√©s\n",
    "    points_morphed = (1.0 - alpha) * ptsA + alpha * ptsB\n",
    "    points_morphed = clamp_points(points_morphed, SIZE, SIZE)\n",
    "\n",
    "    # Triangulation de Delaunay\n",
    "    rect = (0, 0, SIZE, SIZE)\n",
    "    subdiv = cv2.Subdiv2D(rect)\n",
    "\n",
    "    for p in points_morphed:\n",
    "        x, y = float(p[0]), float(p[1])\n",
    "        if 0 <= x < SIZE and 0 <= y < SIZE:\n",
    "            subdiv.insert((x, y))\n",
    "\n",
    "    triangle_list = subdiv.getTriangleList()\n",
    "\n",
    "    tri_indices = []\n",
    "    for t in triangle_list:\n",
    "        tri_pts = [(t[0], t[1]), (t[2], t[3]), (t[4], t[5])]\n",
    "        inds = []\n",
    "        valid = True\n",
    "        for p in tri_pts:\n",
    "            idx = find_point_index(points_morphed, p, tol=5.0)\n",
    "            if idx is None:\n",
    "                valid = False\n",
    "                break\n",
    "            inds.append(idx)\n",
    "        if valid and len(set(inds)) == 3:\n",
    "            tri_indices.append(tuple(inds))\n",
    "\n",
    "    tri_indices = list(set(tri_indices))\n",
    "\n",
    "    # Morphing\n",
    "    img_morphed = np.zeros_like(imgA_color, dtype=np.float32)\n",
    "\n",
    "    for tri in tri_indices:\n",
    "        i1, i2, i3 = tri\n",
    "        tA = [ptsA[i1], ptsA[i2], ptsA[i3]]\n",
    "        tB = [ptsB[i1], ptsB[i2], ptsB[i3]]\n",
    "        tM = [points_morphed[i1], points_morphed[i2], points_morphed[i3]]\n",
    "\n",
    "        if not (triangle_completely_inside(tA, SIZE, SIZE) and \n",
    "                triangle_completely_inside(tB, SIZE, SIZE) and \n",
    "                triangle_completely_inside(tM, SIZE, SIZE)):\n",
    "            continue\n",
    "\n",
    "        morph_triangle(imgA_color, imgB_color, img_morphed, tA, tB, tM, alpha)\n",
    "\n",
    "    return np.clip(img_morphed, 0, 255).astype(np.uint8)\n",
    "\n",
    "# ------------------ G√©n√©ration des morphings ------------------\n",
    "\n",
    "def generate_all_morphings():\n",
    "    \"\"\"G√©n√®re tous les morphings possibles\"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_people = len(unique_labels)\n",
    "    \n",
    "    # Calculer le nombre total de combinaisons\n",
    "    if MODE == \"all\":\n",
    "        total_combinations = n_people * (n_people - 1) // 2\n",
    "        print(f\"\\nüî¢ G√©n√©ration de TOUS les morphings\")\n",
    "        print(f\"   - Nombre de personnes: {n_people}\")\n",
    "        print(f\"   - Combinaisons possibles: {total_combinations}\")\n",
    "        print(f\"   - Alpha values: {len(ALPHA_VALUES)}\")\n",
    "        print(f\"   - Total morphings √† g√©n√©rer: {total_combinations * len(ALPHA_VALUES)}\")\n",
    "        \n",
    "        if total_combinations > 10000:\n",
    "            print(f\"\\n‚ö†Ô∏è  ATTENTION: Cela va g√©n√©rer {total_combinations * len(ALPHA_VALUES)} images!\")\n",
    "            print(f\"   Cela peut prendre plusieurs heures...\")\n",
    "            response = input(\"   Continuer? (o/n): \")\n",
    "            if response.lower() not in ['o', 'oui', 'y', 'yes']:\n",
    "                print(\"   Annul√©.\")\n",
    "                return\n",
    "        \n",
    "        person_pairs = list(combinations(unique_labels, 2))\n",
    "        \n",
    "    elif MODE == \"sample\":\n",
    "        total_combinations = min(NUM_SAMPLES, n_people * (n_people - 1) // 2)\n",
    "        print(f\"\\nüî¢ G√©n√©ration d'un √©chantillon de morphings\")\n",
    "        print(f\"   - Nombre de personnes: {n_people}\")\n",
    "        print(f\"   - Morphings √† g√©n√©rer: {total_combinations * len(ALPHA_VALUES)}\")\n",
    "        \n",
    "        all_pairs = list(combinations(unique_labels, 2))\n",
    "        person_pairs = [all_pairs[i] for i in np.random.choice(len(all_pairs), total_combinations, replace=False)]\n",
    "        \n",
    "    elif MODE == \"per_person\":\n",
    "        total_combinations = n_people\n",
    "        print(f\"\\nüî¢ G√©n√©ration d'un morphing par personne\")\n",
    "        print(f\"   - Nombre de personnes: {n_people}\")\n",
    "        print(f\"   - Total morphings: {total_combinations * len(ALPHA_VALUES)}\")\n",
    "        \n",
    "        person_pairs = []\n",
    "        for i, person_a in enumerate(unique_labels):\n",
    "            person_b = unique_labels[(i + 1) % n_people]\n",
    "            person_pairs.append((person_a, person_b))\n",
    "    \n",
    "    print(f\"\\nüöÄ D√©but de la g√©n√©ration...\")\n",
    "    \n",
    "    morphed_images = []\n",
    "    metadata = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # G√©n√©rer les morphings avec barre de progression\n",
    "    for idx, (person_a, person_b) in enumerate(tqdm(person_pairs, desc=\"Morphing\")):\n",
    "        # R√©cup√©rer les images\n",
    "        imgs_a = images[labels == person_a]\n",
    "        imgs_b = images[labels == person_b]\n",
    "        \n",
    "        # S√©lectionner une image al√©atoire pour chaque personne\n",
    "        imgA = imgs_a[np.random.randint(len(imgs_a))]\n",
    "        imgB = imgs_b[np.random.randint(len(imgs_b))]\n",
    "        \n",
    "        name_a = target_names[person_a]\n",
    "        name_b = target_names[person_b]\n",
    "        \n",
    "        # G√©n√©rer pour chaque alpha\n",
    "        for alpha in ALPHA_VALUES:\n",
    "            try:\n",
    "                img_morphed = morph_faces(imgA, imgB, alpha=alpha)\n",
    "                \n",
    "                # Sauvegarder individuellement si demand√©\n",
    "                if SAVE_INDIVIDUAL:\n",
    "                    filename = f\"morph_{idx:04d}_{name_a[:10]}_{name_b[:10]}_alpha{alpha:.2f}.png\"\n",
    "                    filepath = OUTPUT_DIR / filename\n",
    "                    cv2.imwrite(str(filepath), img_morphed)\n",
    "                \n",
    "                # Ajouter √† la liste pour la grille\n",
    "                morphed_images.append(img_morphed)\n",
    "                metadata.append({\n",
    "                    'person_a': name_a,\n",
    "                    'person_b': name_b,\n",
    "                    'alpha': alpha,\n",
    "                    'index': idx\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è  Erreur sur {name_a} + {name_b}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ G√©n√©ration termin√©e!\")\n",
    "    print(f\"   - Temps √©coul√©: {elapsed_time:.1f} secondes ({elapsed_time/60:.1f} minutes)\")\n",
    "    print(f\"   - Images g√©n√©r√©es: {len(morphed_images)}\")\n",
    "    print(f\"   - Vitesse: {len(morphed_images)/elapsed_time:.2f} images/seconde\")\n",
    "    \n",
    "    # Cr√©er une grille si demand√©\n",
    "    if CREATE_GRID and len(morphed_images) > 0:\n",
    "        print(f\"\\nüìä Cr√©ation de la grille...\")\n",
    "        create_grid_visualization(morphed_images, metadata)\n",
    "    \n",
    "    print(f\"\\nüíæ Tous les fichiers sont dans: {OUTPUT_DIR}\")\n",
    "\n",
    "def create_grid_visualization(morphed_images, metadata):\n",
    "    \"\"\"Cr√©e une grille de visualisation\"\"\"\n",
    "    rows, cols = GRID_SIZE\n",
    "    n_images = min(len(morphed_images), rows * cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    axes = axes.flatten() if rows * cols > 1 else [axes]\n",
    "    \n",
    "    for i in range(rows * cols):\n",
    "        if i < n_images:\n",
    "            img = morphed_images[i]\n",
    "            meta = metadata[i]\n",
    "            axes[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            title = f\"{meta['person_a'][:10]}\\n+\\n{meta['person_b'][:10]}\"\n",
    "            axes[i].set_title(title, fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    grid_file = OUTPUT_DIR / f\"morphing_grid_{rows}x{cols}.png\"\n",
    "    plt.savefig(grid_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ‚úì Grille sauvegard√©e: {grid_file}\")\n",
    "    plt.close()\n",
    "\n",
    "# ------------------ Programme Principal ------------------\n",
    "def main():\n",
    "    # D√©placer la d√©claration global au d√©but de la fonction\n",
    "    global MODE, NUM_SAMPLES, ALPHA_VALUES\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚öôÔ∏è  PARAM√àTRES\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nVoulez-vous modifier les param√®tres? (d√©faut: non)\")\n",
    "    print(f\"Configuration actuelle:\")\n",
    "    print(f\"  - Mode: {MODE}\")\n",
    "    print(f\"  - Alpha values: {ALPHA_VALUES}\")\n",
    "    if MODE == \"sample\":\n",
    "        print(f\"  - Nombre d'√©chantillons: {NUM_SAMPLES}\")\n",
    "    \n",
    "    response = input(\"\\nModifier? (o/N): \")\n",
    "    \n",
    "    if response.lower() in ['o', 'oui', 'y', 'yes']:\n",
    "        print(\"\\nMode de g√©n√©ration:\")\n",
    "        print(\"  1. all - TOUTES les combinaisons (peut √™tre tr√®s long!)\")\n",
    "        print(\"  2. sample - Un √©chantillon al√©atoire\")\n",
    "        print(\"  3. per_person - Un morphing par personne\")\n",
    "        \n",
    "        mode_choice = input(\"Choix (1/2/3): \").strip()\n",
    "        if mode_choice == \"1\":\n",
    "            MODE = \"all\"\n",
    "        elif mode_choice == \"2\":\n",
    "            MODE = \"sample\"\n",
    "            NUM_SAMPLES = int(input(\"Nombre d'√©chantillons (ex: 100): \"))\n",
    "        elif mode_choice == \"3\":\n",
    "            MODE = \"per_person\"\n",
    "        \n",
    "        alpha_input = input(\"Alpha values (ex: 0.3,0.5,0.7): \").strip()\n",
    "        if alpha_input:\n",
    "            ALPHA_VALUES = [float(x) for x in alpha_input.split(',')]\n",
    "    \n",
    "    # G√©n√©rer les morphings\n",
    "    generate_all_morphings()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ TERMIN√â!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nüìÇ Vos morphings sont dans: {OUTPUT_DIR}\")\n",
    "    print(f\"\\nüí° Astuce: Utilisez MODE='sample' pour des tests rapides!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7261976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Cr√©ation de la structure du dataset...\n",
      "   - Trouv√© 562 images morph√©es\n",
      "   - G√©n√©ration d'images r√©elles...\n",
      "‚úÖ Dataset cr√©√©!\n",
      "   - Images r√©elles: 562\n",
      "   - Images morph√©es: 562\n",
      "   - Total: 1124\n",
      "\n",
      "üéØ Dataset pr√™t pour l'entra√Ænement!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ------------------ Organisation du Dataset ------------------\n",
    "def create_training_dataset():\n",
    "    \"\"\"\n",
    "    Organise les donn√©es pour l'entra√Ænement du mod√®le\n",
    "    Structure requise:\n",
    "    ./dataset/\n",
    "    ‚îú‚îÄ‚îÄ real/          # Images r√©elles\n",
    "    ‚îî‚îÄ‚îÄ morph/         # Images morph√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìÅ Cr√©ation de la structure du dataset...\")\n",
    "    \n",
    "    # Cr√©er la structure de dossiers\n",
    "    dataset_dir = Path(\"./dataset\")\n",
    "    real_dir = dataset_dir / \"real\"\n",
    "    morph_dir = dataset_dir / \"morph\"\n",
    "    \n",
    "    # Supprimer et recr√©er les dossiers\n",
    "    if dataset_dir.exists():\n",
    "        shutil.rmtree(dataset_dir)\n",
    "    \n",
    "    dataset_dir.mkdir(exist_ok=True)\n",
    "    real_dir.mkdir(exist_ok=True)\n",
    "    morph_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Copier les images morph√©es\n",
    "    morphing_results = Path(\"./morphing_results\")\n",
    "    if morphing_results.exists():\n",
    "        morph_files = list(morphing_results.glob(\"*.png\"))\n",
    "        print(f\"   - Trouv√© {len(morph_files)} images morph√©es\")\n",
    "        \n",
    "        for i, morph_file in enumerate(morph_files):\n",
    "            dest = morph_dir / f\"morph_{i:04d}.png\"\n",
    "            shutil.copy2(morph_file, dest)\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Dossier morphing_results introuvable!\")\n",
    "        print(\"   Ex√©cutez d'abord la g√©n√©ration de morphings\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Cr√©er des images \"r√©elles\" √† partir du dataset LFW\n",
    "    print(\"   - G√©n√©ration d'images r√©elles...\")\n",
    "    \n",
    "    # Utiliser quelques images du dataset LFW comme \"vraies\" images\n",
    "    num_real_images = len(morph_files)  # M√™me nombre que les morphings\n",
    "    selected_indices = np.random.choice(len(images), num_real_images, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        # Redimensionner et sauvegarder l'image originale\n",
    "        img_original = images[idx]\n",
    "        img_resized = cv2.resize(img_original, (SIZE, SIZE))\n",
    "        \n",
    "        # Convertir en uint8 si n√©cessaire\n",
    "        if img_resized.dtype != np.uint8:\n",
    "            img_resized = (img_resized * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convertir en couleur pour √™tre coh√©rent avec les morphings\n",
    "        img_color = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        # Sauvegarder\n",
    "        dest = real_dir / f\"real_{i:04d}.png\"\n",
    "        cv2.imwrite(str(dest), img_color)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset cr√©√©!\")\n",
    "    print(f\"   - Images r√©elles: {len(list(real_dir.glob('*.png')))}\")\n",
    "    print(f\"   - Images morph√©es: {len(list(morph_dir.glob('*.png')))}\")\n",
    "    print(f\"   - Total: {len(list(real_dir.glob('*.png'))) + len(list(morph_dir.glob('*.png')))}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Cr√©er le dataset\n",
    "success = create_training_dataset()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéØ Dataset pr√™t pour l'entra√Ænement!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Probl√®me lors de la cr√©ation du dataset\")\n",
    "    print(\"   Assurez-vous d'avoir d'abord ex√©cut√© la g√©n√©ration de morphings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ee4ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Structure du projet cr√©√©e\n",
      "   - Dossier principal: mia_project\n",
      "   - 50 identit√©s √† classifier\n",
      "   - Taille d'images: (128, 128)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# PROJET: Face Blending et Attaques par Inf√©rence d'Appartenance (MIA)\n",
    "# Bas√© sur: Ghorbel et al. (2024) & Shokri et al. (2017)\n",
    "# ==================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import ResNet50, InceptionV3, MobileNetV2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration globale\n",
    "class Config:\n",
    "    \"\"\"Configuration centralis√©e pour le projet MIA\"\"\"\n",
    "    \n",
    "    # Param√®tres des donn√©es\n",
    "    IMG_SIZE = (128, 128)\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_CLASSES = 50  # Nombre d'identit√©s √† classifier\n",
    "    \n",
    "    # Param√®tres d'entra√Ænement\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Param√®tres du Face Blending\n",
    "    BLEND_RATIOS = [0.3, 0.5, 0.7]  # Diff√©rents ratios de m√©lange\n",
    "    AUGMENTATION_FACTOR = 3  # Combien d'images blend√©es par identit√©\n",
    "    \n",
    "    # Param√®tres MIA\n",
    "    MIA_EPOCHS = 30\n",
    "    MIA_BATCH_SIZE = 64\n",
    "    SHADOW_MODELS = 5  # Nombre de mod√®les shadow pour MIA\n",
    "    \n",
    "    # Chemins\n",
    "    BASE_DIR = Path(\"./mia_project\")\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    MODELS_DIR = BASE_DIR / \"models\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Cr√©er les dossiers\n",
    "        for directory in [self.BASE_DIR, self.DATA_DIR, self.MODELS_DIR, self.RESULTS_DIR]:\n",
    "            directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config = Config()\n",
    "print(\"üìÅ Structure du projet cr√©√©e\")\n",
    "print(f\"   - Dossier principal: {config.BASE_DIR}\")\n",
    "print(f\"   - {config.NUM_CLASSES} identit√©s √† classifier\")\n",
    "print(f\"   - Taille d'images: {config.IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda5ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Structure du projet cr√©√©e\n",
      "   - Dossier principal: mia_project\n",
      "   - 50 identit√©s √† classifier\n",
      "   - Taille d'images: (128, 128)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# PROJET: Face Blending et Attaques par Inf√©rence d'Appartenance (MIA)\n",
    "# Bas√© sur: Ghorbel et al. (2024) & Shokri et al. (2017)\n",
    "# ==================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import ResNet50, InceptionV3, MobileNetV2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration globale\n",
    "class Config:\n",
    "    \"\"\"Configuration centralis√©e pour le projet MIA\"\"\"\n",
    "    \n",
    "    # Param√®tres des donn√©es\n",
    "    IMG_SIZE = (128, 128)\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_CLASSES = 50  # Nombre d'identit√©s √† classifier\n",
    "    \n",
    "    # Param√®tres d'entra√Ænement\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Param√®tres du Face Blending\n",
    "    BLEND_RATIOS = [0.3, 0.5, 0.7]  # Diff√©rents ratios de m√©lange\n",
    "    AUGMENTATION_FACTOR = 3  # Combien d'images blend√©es par identit√©\n",
    "    \n",
    "    # Param√®tres MIA\n",
    "    MIA_EPOCHS = 30\n",
    "    MIA_BATCH_SIZE = 64\n",
    "    SHADOW_MODELS = 5  # Nombre de mod√®les shadow pour MIA\n",
    "    \n",
    "    # Chemins\n",
    "    BASE_DIR = Path(\"./mia_project\")\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    MODELS_DIR = BASE_DIR / \"models\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Cr√©er les dossiers\n",
    "        for directory in [self.BASE_DIR, self.DATA_DIR, self.MODELS_DIR, self.RESULTS_DIR]:\n",
    "            directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config = Config()\n",
    "print(\"üìÅ Structure du projet cr√©√©e\")\n",
    "print(f\"   - Dossier principal: {config.BASE_DIR}\")\n",
    "print(f\"   - {config.NUM_CLASSES} identit√©s √† classifier\")\n",
    "print(f\"   - Taille d'images: {config.IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9b0a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Chargement du dataset LFW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2527/2527 [00:00<00:00, 5256.18it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 474. MiB for an array with shape (2527, 128, 128, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Initialisation du g√©n√©rateur de donn√©es\u001b[39;00m\n\u001b[0;32m    161\u001b[0m data_generator \u001b[38;5;241m=\u001b[39m FaceBlendingDataGenerator(config)\n\u001b[1;32m--> 162\u001b[0m original_data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lfw_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mFaceBlendingDataGenerator.load_lfw_subset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     img_normalized \u001b[38;5;241m=\u001b[39m img_resized\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     50\u001b[0m     processed_images\u001b[38;5;241m.\u001b[39mappend(img_normalized)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Dataset charg√©:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 474. MiB for an array with shape (2527, 128, 128, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "class FaceBlendingDataGenerator:\n",
    "    \"\"\"\n",
    "    G√©n√©rateur de donn√©es avec Face Blending pour am√©liorer la confidentialit√©\n",
    "    Bas√© sur Ghorbel et al. (2024)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.original_data = None\n",
    "        self.blended_data = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def load_lfw_subset(self):\n",
    "        \"\"\"Charge un sous-ensemble du dataset LFW avec des identit√©s s√©lectionn√©es\"\"\"\n",
    "        print(\"\\nüì• Chargement du dataset LFW...\")\n",
    "        \n",
    "        from sklearn.datasets import fetch_lfw_people\n",
    "        \n",
    "        # Charger avec plus d'images par personne pour avoir assez de donn√©es\n",
    "        lfw_people = fetch_lfw_people(\n",
    "            min_faces_per_person=20,  # Au moins 20 images par personne\n",
    "            resize=0.5,\n",
    "            color=False\n",
    "        )\n",
    "        \n",
    "        # S√©lectionner les N premi√®res identit√©s\n",
    "        unique_labels = np.unique(lfw_people.target)[:self.config.NUM_CLASSES]\n",
    "        \n",
    "        # Filtrer les donn√©es\n",
    "        mask = np.isin(lfw_people.target, unique_labels)\n",
    "        images = lfw_people.images[mask]\n",
    "        labels = lfw_people.target[mask]\n",
    "        \n",
    "        # Remapper les labels de 0 √† N-1\n",
    "        label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
    "        labels = np.array([label_mapping[label] for label in labels])\n",
    "        \n",
    "        # Redimensionner et normaliser\n",
    "        processed_images = []\n",
    "        for img in tqdm(images, desc=\"Preprocessing\"):\n",
    "            # Redimensionner\n",
    "            img_resized = cv2.resize(img, self.config.IMG_SIZE)\n",
    "            \n",
    "            # Convertir en couleur\n",
    "            if len(img_resized.shape) == 2:\n",
    "                img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "            # Normaliser\n",
    "            img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "            processed_images.append(img_normalized)\n",
    "        \n",
    "        self.original_data = np.array(processed_images)\n",
    "        self.labels = labels\n",
    "        \n",
    "        print(f\"‚úÖ Dataset charg√©:\")\n",
    "        print(f\"   - {len(self.original_data)} images\")\n",
    "        print(f\"   - {self.config.NUM_CLASSES} identit√©s\")\n",
    "        print(f\"   - Forme: {self.original_data.shape}\")\n",
    "        \n",
    "        return self.original_data, self.labels\n",
    "    \n",
    "    def create_blended_faces(self, alpha_values=None):\n",
    "        \"\"\"\n",
    "        Cr√©e des visages m√©lang√©s (Face Blending) inter-classe\n",
    "        \"\"\"\n",
    "        if alpha_values is None:\n",
    "            alpha_values = self.config.BLEND_RATIOS\n",
    "            \n",
    "        print(f\"\\nüé≠ G√©n√©ration de Face Blending...\")\n",
    "        print(f\"   - Ratios de m√©lange: {alpha_values}\")\n",
    "        \n",
    "        blended_images = []\n",
    "        blended_labels = []\n",
    "        blend_metadata = []\n",
    "        \n",
    "        for class_id in tqdm(range(self.config.NUM_CLASSES), desc=\"Blending identit√©s\"):\n",
    "            # Images de cette classe\n",
    "            class_mask = self.labels == class_id\n",
    "            class_images = self.original_data[class_mask]\n",
    "            \n",
    "            # Pour chaque ratio de m√©lange\n",
    "            for alpha in alpha_values:\n",
    "                # Pour chaque facteur d'augmentation\n",
    "                for aug_idx in range(self.config.AUGMENTATION_FACTOR):\n",
    "                    # S√©lectionner deux images al√©atoirement\n",
    "                    if len(class_images) >= 2:\n",
    "                        idx1, idx2 = np.random.choice(len(class_images), 2, replace=False)\n",
    "                        img1 = class_images[idx1]\n",
    "                        img2 = class_images[idx2]\n",
    "                        \n",
    "                        # Face blending simple (m√©lange lin√©aire)\n",
    "                        blended_img = alpha * img1 + (1 - alpha) * img2\n",
    "                        \n",
    "                        # Ajouter du bruit l√©ger pour plus de r√©alisme\n",
    "                        noise = np.random.normal(0, 0.01, blended_img.shape)\n",
    "                        blended_img = np.clip(blended_img + noise, 0, 1)\n",
    "                        \n",
    "                        blended_images.append(blended_img)\n",
    "                        blended_labels.append(class_id)  # Garde le label de la classe\n",
    "                        \n",
    "                        blend_metadata.append({\n",
    "                            'class_id': class_id,\n",
    "                            'alpha': alpha,\n",
    "                            'aug_idx': aug_idx,\n",
    "                            'source_indices': [idx1, idx2]\n",
    "                        })\n",
    "        \n",
    "        self.blended_data = np.array(blended_images)\n",
    "        self.blend_metadata = blend_metadata\n",
    "        \n",
    "        print(f\"‚úÖ Face Blending termin√©:\")\n",
    "        print(f\"   - {len(blended_images)} images blend√©es g√©n√©r√©es\")\n",
    "        print(f\"   - Facteur d'augmentation: {len(blended_images) / len(self.original_data):.1f}x\")\n",
    "        \n",
    "        return self.blended_data, np.array(blended_labels)\n",
    "    \n",
    "    def prepare_training_data(self, include_blended=True, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Pr√©pare les donn√©es d'entra√Ænement en combinant originales et blend√©es\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä Pr√©paration des donn√©es d'entra√Ænement...\")\n",
    "        \n",
    "        if include_blended and self.blended_data is not None:\n",
    "            # Combiner donn√©es originales et blend√©es\n",
    "            all_images = np.vstack([self.original_data, self.blended_data])\n",
    "            original_labels = self.labels\n",
    "            blended_labels = np.array([meta['class_id'] for meta in self.blend_metadata])\n",
    "            all_labels = np.hstack([original_labels, blended_labels])\n",
    "            \n",
    "            # Marquer quelles images sont blend√©es (pour MIA)\n",
    "            is_blended = np.hstack([\n",
    "                np.zeros(len(self.original_data), dtype=bool),  # Originales = False\n",
    "                np.ones(len(self.blended_data), dtype=bool)     # Blend√©es = True\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            all_images = self.original_data\n",
    "            all_labels = self.labels\n",
    "            is_blended = np.zeros(len(self.original_data), dtype=bool)\n",
    "        \n",
    "        # Division train/test stratifi√©e\n",
    "        X_train, X_test, y_train, y_test, blend_train, blend_test = train_test_split(\n",
    "            all_images, all_labels, is_blended,\n",
    "            test_size=test_size,\n",
    "            stratify=all_labels,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Donn√©es pr√©par√©es:\")\n",
    "        print(f\"   - Train: {len(X_train)} images ({np.sum(blend_train)} blend√©es)\")\n",
    "        print(f\"   - Test: {len(X_test)} images ({np.sum(blend_test)} blend√©es)\")\n",
    "        print(f\"   - Classes: {len(np.unique(all_labels))}\")\n",
    "        \n",
    "        return {\n",
    "            'X_train': X_train, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_test': y_test,\n",
    "            'is_blended_train': blend_train, 'is_blended_test': blend_test\n",
    "        }\n",
    "\n",
    "# Initialisation du g√©n√©rateur de donn√©es\n",
    "data_generator = FaceBlendingDataGenerator(config)\n",
    "original_data, labels = data_generator.load_lfw_subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è  Construction du mod√®le RESNET50...\n"
     ]
    }
   ],
   "source": [
    "class AdvancedCNNClassifier:\n",
    "    \"\"\"\n",
    "    Mod√®le CNN avanc√© pour classification d'identit√©s avec transfer learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, architecture='resnet50'):\n",
    "        self.config = config\n",
    "        self.architecture = architecture\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Construit le mod√®le avec transfer learning\"\"\"\n",
    "        print(f\"\\nüèóÔ∏è  Construction du mod√®le {self.architecture.upper()}...\")\n",
    "        \n",
    "        # Mod√®le de base pr√©-entra√Æn√©\n",
    "        if self.architecture == 'resnet50':\n",
    "            base_model = ResNet50(\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=(*self.config.IMG_SIZE, 3)\n",
    "            )\n",
    "        elif self.architecture == 'inception':\n",
    "            base_model = InceptionV3(\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=(*self.config.IMG_SIZE, 3)\n",
    "            )\n",
    "        elif self.architecture == 'mobilenet':\n",
    "            base_model = MobileNetV2(\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=(*self.config.IMG_SIZE, 3)\n",
    "            )\n",
    "        \n",
    "        # Geler les couches de base initialement\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Ajouter les couches de classification\n",
    "        self.model = models.Sequential([\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(self.config.NUM_CLASSES, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Compilation\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy', 'top_3_accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Mod√®le construit:\")\n",
    "        print(f\"   - Architecture: {self.architecture}\")\n",
    "        print(f\"   - Param√®tres total: {self.model.count_params():,}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, fine_tune=True):\n",
    "        \"\"\"Entra√Æne le mod√®le avec fine-tuning optionnel\"\"\"\n",
    "        print(f\"\\nüöÄ D√©but de l'entra√Ænement...\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks_list = [\n",
    "            callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7\n",
    "            ),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                self.config.MODELS_DIR / f\"{self.architecture}_best.h5\",\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Phase 1: Entra√Ænement avec base gel√©e\n",
    "        print(\"üìö Phase 1: Entra√Ænement des couches de classification...\")\n",
    "        history1 = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=self.config.EPOCHS // 2,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Phase 2: Fine-tuning si demand√©\n",
    "        if fine_tune:\n",
    "            print(\"üîß Phase 2: Fine-tuning...\")\n",
    "            \n",
    "            # D√©geler les derni√®res couches du mod√®le de base\n",
    "            base_model = self.model.layers[0]\n",
    "            base_model.trainable = True\n",
    "            \n",
    "            # Geler les premi√®res couches (garder les features de bas niveau)\n",
    "            for layer in base_model.layers[:-50]:  # Ajuster selon l'architecture\n",
    "                layer.trainable = False\n",
    "            \n",
    "            # R√©compiler avec un learning rate plus bas\n",
    "            self.model.compile(\n",
    "                optimizer=optimizers.Adam(learning_rate=self.config.LEARNING_RATE/10),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy', 'top_3_accuracy']\n",
    "            )\n",
    "            \n",
    "            # Continuer l'entra√Ænement\n",
    "            history2 = self.model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=self.config.EPOCHS // 2,\n",
    "                batch_size=self.config.BATCH_SIZE,\n",
    "                callbacks=callbacks_list,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Combiner les historiques\n",
    "            for key in history1.history:\n",
    "                history1.history[key].extend(history2.history[key])\n",
    "        \n",
    "        self.history = history1\n",
    "        print(\"‚úÖ Entra√Ænement termin√©!\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"√âvalue le mod√®le et retourne les m√©triques d√©taill√©es\"\"\"\n",
    "        print(\"\\nüìä √âvaluation du mod√®le...\")\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        y_pred_probs = self.model.predict(X_test, batch_size=self.config.BATCH_SIZE)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        # M√©triques de base\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        \n",
    "        # Rapport de classification\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        print(f\"‚úÖ R√©sultats d'√©valuation:\")\n",
    "        print(f\"   - Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   - Macro F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "        print(f\"   - Weighted F1: {report['weighted avg']['f1-score']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_probs,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "classifier = AdvancedCNNClassifier(config, architecture='resnet50')\n",
    "model = classifier.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e962dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FaceBlendingDataGenerator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.original_data = None\n",
    "        self.blended_data = None\n",
    "        self.labels = None\n",
    "\n",
    "    def load_lfw_subset(self):\n",
    "        from sklearn.datasets import fetch_lfw_people\n",
    "        print(\"\\nüì• Chargement du dataset LFW (Optimis√© RAM)...\")\n",
    "        \n",
    "        lfw_people = fetch_lfw_people(min_faces_per_person=20, resize=0.5, color=False)\n",
    "        unique_labels = np.unique(lfw_people.target)[:self.config.NUM_CLASSES]\n",
    "        mask = np.isin(lfw_people.target, unique_labels)\n",
    "        images = lfw_people.images[mask]\n",
    "        raw_labels = lfw_people.target[mask]\n",
    "        \n",
    "        label_mapping = {old: new for new, old in enumerate(unique_labels)}\n",
    "        self.labels = np.array([label_mapping[l] for l in raw_labels])\n",
    "        \n",
    "        n_imgs = len(images)\n",
    "        self.original_data = np.zeros((n_imgs, *self.config.IMG_SIZE, 3), dtype=np.float32)\n",
    "        \n",
    "        for i, img in enumerate(tqdm(images, desc=\"Preprocessing\")):\n",
    "            img_res = cv2.resize(img, self.config.IMG_SIZE)\n",
    "            if len(img_res.shape) == 2:\n",
    "                img_res = cv2.cvtColor(img_res, cv2.COLOR_GRAY2RGB)\n",
    "            self.original_data[i] = img_res.astype(np.float32) / 255.0\n",
    "            \n",
    "        del images\n",
    "        gc.collect()\n",
    "        return self.original_data, self.labels\n",
    "\n",
    "    def create_blended_faces(self):\n",
    "        # CORRECTION ICI : Ajout du pr√©fixe r pour √©viter SyntaxWarning\n",
    "        r\"\"\"M√©lange lin√©aire : $$I_{blend} = \\alpha \\cdot I_1 + (1 - \\alpha) \\cdot I_2$$\"\"\"\n",
    "        alpha_values = self.config.BLEND_RATIOS\n",
    "        print(f\"\\nüé≠ G√©n√©ration de Face Blending...\")\n",
    "        \n",
    "        blended_list = []\n",
    "        blended_labels = []\n",
    "        \n",
    "        for class_id in tqdm(range(self.config.NUM_CLASSES), desc=\"Blending\"):\n",
    "            class_mask = self.labels == class_id\n",
    "            class_images = self.original_data[class_mask]\n",
    "            \n",
    "            if len(class_images) < 2: continue\n",
    "\n",
    "            for alpha in alpha_values:\n",
    "                for _ in range(self.config.AUGMENTATION_FACTOR):\n",
    "                    idx1, idx2 = np.random.choice(len(class_images), 2, replace=False)\n",
    "                    blend = alpha * class_images[idx1] + (1 - alpha) * class_images[idx2]\n",
    "                    blend = np.clip(blend + np.random.normal(0, 0.01, blend.shape), 0, 1)\n",
    "                    blended_list.append(blend)\n",
    "                    blended_labels.append(class_id)\n",
    "\n",
    "        self.blended_data = np.array(blended_list, dtype=np.float32)\n",
    "        self.blended_labels = np.array(blended_labels)\n",
    "        print(f\"‚úÖ Blending termin√© : {len(self.blended_data)} images g√©n√©r√©es.\")\n",
    "        return self.blended_data, self.blended_labels\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        X = np.concatenate([self.original_data, self.blended_data], axis=0)\n",
    "        y = np.concatenate([self.labels, self.blended_labels], axis=0)\n",
    "        return train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b0530f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _pywrap_dtensor_device: Le module sp√©cifi√© est introuvable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MobileNetV2\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models, optimizers, callbacks\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a TF control dependency on the return values of a function.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m  If the function had no return value, a no-op context is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    A context manager.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_spec\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_spec_registry\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flags\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m d_api\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m record\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\dtensor\\python\\api.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional, Sequence\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_device\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_dtensor_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout \u001b[38;5;28;01mas\u001b[39;00m layout_lib\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\dtensor\\python\\dtensor_device.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout \u001b[38;5;28;01mas\u001b[39;00m layout_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_dtensor_device\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\dtensor\\python\\layout.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_pb2\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_dtensor_device\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device \u001b[38;5;28;01mas\u001b[39;00m tf_device\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_dtensor_device: Le module sp√©cifi√© est introuvable."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "class AdvancedCNNClassifier:\n",
    "    def __init__(self, config, architecture='mobilenet'):\n",
    "        self.config = config\n",
    "        self.architecture = architecture\n",
    "        # Nettoyage pr√©ventif\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    def build_model(self):\n",
    "        print(f\"\\nüèóÔ∏è Construction du mod√®le {self.architecture.upper()}...\")\n",
    "        \n",
    "        # Utilisation de MobileNetV2 (Beaucoup plus l√©ger que ResNet)\n",
    "        base_model = MobileNetV2(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(*self.config.IMG_SIZE, 3)\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "\n",
    "        self.model = models.Sequential([\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(256, activation='relu'), # R√©duit de 512 √† 256 pour gagner de la RAM\n",
    "            layers.Dense(self.config.NUM_CLASSES, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Mod√®le pr√™t. Param√®tres : {self.model.count_params():,}\")\n",
    "        return self.model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        # Utilisation de batch_size plus petit si √ßa plante encore\n",
    "        return self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=self.config.EPOCHS,\n",
    "            batch_size=self.config.BATCH_SIZE, # Essaye 16 si 32 √©choue\n",
    "            callbacks=[\n",
    "                callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cca3bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Configuration mise √† jour:\n",
      "   - Taille d'image: (64, 64)\n",
      "   - Nombre de classes: 15\n",
      "   - Ratios de blend: [0.3, 0.7]\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# SOLUTION COMPL√àTE ET OPTIMIS√âE POUR LE PROJET MIA\n",
    "# ==================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration optimis√©e\n",
    "class OptimizedConfig:\n",
    "    # Param√®tres adapt√©s √† votre hardware\n",
    "    IMG_SIZE = (64, 64)     # Plus petit = moins de RAM\n",
    "    NUM_CLASSES = 15        # Augment√© mais raisonnable\n",
    "    MIN_SAMPLES_PER_CLASS = 10  # Minimum d'√©chantillons par classe\n",
    "    \n",
    "    # Face Blending\n",
    "    BLEND_RATIOS = [0.3, 0.7]  # Deux ratios pour plus de diversit√©\n",
    "    AUGMENTATION_FACTOR = 3    # Plus d'augmentation\n",
    "    \n",
    "    # ML Parameters\n",
    "    PCA_COMPONENTS = 100       # Plus de composantes PCA\n",
    "    TEST_SIZE = 0.25          # 75% train, 25% test\n",
    "    \n",
    "    # Chemins\n",
    "    BASE_DIR = Path(\"./mia_project_fixed\")\n",
    "    RESULTS_DIR = BASE_DIR / \"results\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.BASE_DIR.mkdir(exist_ok=True)\n",
    "        self.RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "config = OptimizedConfig()\n",
    "print(f\"üìù Configuration mise √† jour:\")\n",
    "print(f\"   - Taille d'image: {config.IMG_SIZE}\")\n",
    "print(f\"   - Nombre de classes: {config.NUM_CLASSES}\")\n",
    "print(f\"   - Ratios de blend: {config.BLEND_RATIOS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb6cae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©but du chargement corrig√©...\n",
      "\n",
      "üì• Chargement √©quilibr√© du dataset LFW (Version Corrig√©e)...\n",
      "   Distribution des classes disponibles:\n",
      "      Classe 0: 19 images\n",
      "      Classe 1: 12 images\n",
      "      Classe 2: 39 images\n",
      "      Classe 3: 35 images\n",
      "      Classe 4: 21 images\n",
      "      Classe 5: 36 images\n",
      "      Classe 6: 15 images\n",
      "      Classe 7: 20 images\n",
      "      Classe 8: 11 images\n",
      "      Classe 9: 12 images\n",
      "\n",
      "‚úÖ Classes s√©lectionn√©es:\n",
      "   Classe 0: 44 images\n",
      "   Classe 1: 48 images\n",
      "   Classe 2: 49 images\n",
      "   Classe 3: 52 images\n",
      "   Classe 4: 52 images\n",
      "   Classe 5: 53 images\n",
      "   Classe 6: 55 images\n",
      "   Classe 7: 60 images\n",
      "   Classe 8: 71 images\n",
      "   Classe 9: 77 images\n",
      "   Classe 10: 109 images\n",
      "   Classe 11: 121 images\n",
      "   Classe 12: 144 images\n",
      "   Classe 13: 236 images\n",
      "   Classe 14: 530 images\n",
      "\n",
      "üîß Preprocessing avec validation corrig√©e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1701/1701 [00:00<00:00, 17893.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset preprocessed:\n",
      "   - Images finales: 0\n",
      "   - Images rejet√©es (uniformes): 1701\n",
      "   - Images rejet√©es (erreurs): 0\n",
      "   - Images valides: 0\n",
      "   - Classes finales: 0\n",
      "   - Forme: (0,)\n",
      "   - M√©moire: ~0.0 MB\n",
      "\n",
      "üé≠ Cr√©ation de Face Blending s√©curis√©...\n",
      "‚ùå Aucune image disponible pour le blending!\n",
      "\n",
      "üìä V√©rification des donn√©es:\n",
      "   - Images originales: 0\n",
      "   - Images blend√©es: 0\n",
      "   - Labels originaux: 0\n",
      "   - Labels blend√©s: 0\n",
      "‚ùå Pas assez de donn√©es pour l'entra√Ænement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class FixedDataLoader:\n",
    "    \"\"\"Chargeur de donn√©es corrig√© avec crit√®res plus appropri√©s\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.class_info = {}\n",
    "        \n",
    "    def load_balanced_lfw(self):\n",
    "        \"\"\"Charge un dataset √©quilibr√© avec validation appropri√©e\"\"\"\n",
    "        print(\"\\nüì• Chargement √©quilibr√© du dataset LFW (Version Corrig√©e)...\")\n",
    "        \n",
    "        # Charger avec param√®tres optimis√©s\n",
    "        lfw_people = fetch_lfw_people(\n",
    "            min_faces_per_person=self.config.MIN_SAMPLES_PER_CLASS,\n",
    "            resize=0.4,\n",
    "            color=False\n",
    "        )\n",
    "        \n",
    "        # Analyser la distribution des classes\n",
    "        unique_labels, counts = np.unique(lfw_people.target, return_counts=True)\n",
    "        print(f\"   Distribution des classes disponibles:\")\n",
    "        for i, (label, count) in enumerate(zip(unique_labels[:10], counts[:10])):\n",
    "            print(f\"      Classe {label}: {count} images\")\n",
    "        \n",
    "        # S√©lectionner les classes avec le plus d'√©chantillons\n",
    "        top_classes = unique_labels[np.argsort(counts)[-self.config.NUM_CLASSES:]]\n",
    "        mask = np.isin(lfw_people.target, top_classes)\n",
    "        \n",
    "        selected_images = lfw_people.images[mask]\n",
    "        selected_labels = lfw_people.target[mask]\n",
    "        \n",
    "        # Remapper les labels\n",
    "        label_mapping = {old: new for new, old in enumerate(top_classes)}\n",
    "        mapped_labels = np.array([label_mapping[label] for label in selected_labels])\n",
    "        \n",
    "        # V√©rifier l'√©quilibre des classes\n",
    "        unique_mapped, mapped_counts = np.unique(mapped_labels, return_counts=True)\n",
    "        print(f\"\\n‚úÖ Classes s√©lectionn√©es:\")\n",
    "        for label, count in zip(unique_mapped, mapped_counts):\n",
    "            print(f\"   Classe {label}: {count} images\")\n",
    "            self.class_info[label] = count\n",
    "        \n",
    "        # Preprocessing avec validation CORRIG√âE\n",
    "        processed_images = []\n",
    "        valid_labels = []\n",
    "        rejected_stats = {\"too_uniform\": 0, \"processing_error\": 0, \"valid\": 0}\n",
    "        \n",
    "        print(\"\\nüîß Preprocessing avec validation corrig√©e...\")\n",
    "        for i, (img, label) in enumerate(tqdm(zip(selected_images, mapped_labels), \n",
    "                                            total=len(selected_images), desc=\"Processing\")):\n",
    "            try:\n",
    "                # Redimensionner\n",
    "                img_resized = cv2.resize(img, self.config.IMG_SIZE)\n",
    "                \n",
    "                # CORRECTION : Crit√®re de qualit√© adapt√© aux images en niveaux de gris (0-255)\n",
    "                img_std = img_resized.std()\n",
    "                if img_std < 2:  # Beaucoup plus permissif (√©tait 10, maintenant 2)\n",
    "                    rejected_stats[\"too_uniform\"] += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Convertir en couleur\n",
    "                if len(img_resized.shape) == 2:\n",
    "                    img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                # Normaliser correctement\n",
    "                img_normalized = img_resized.astype(np.uint8)  # Garder en uint8 directement\n",
    "                \n",
    "                processed_images.append(img_normalized)\n",
    "                valid_labels.append(label)\n",
    "                rejected_stats[\"valid\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                rejected_stats[\"processing_error\"] += 1\n",
    "                continue\n",
    "        \n",
    "        final_images = np.array(processed_images, dtype=np.uint8)\n",
    "        final_labels = np.array(valid_labels)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset preprocessed:\")\n",
    "        print(f\"   - Images finales: {len(final_images)}\")\n",
    "        print(f\"   - Images rejet√©es (uniformes): {rejected_stats['too_uniform']}\")\n",
    "        print(f\"   - Images rejet√©es (erreurs): {rejected_stats['processing_error']}\")\n",
    "        print(f\"   - Images valides: {rejected_stats['valid']}\")\n",
    "        print(f\"   - Classes finales: {len(np.unique(final_labels))}\")\n",
    "        print(f\"   - Forme: {final_images.shape}\")\n",
    "        print(f\"   - M√©moire: ~{final_images.nbytes / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return final_images, final_labels\n",
    "    \n",
    "    def create_safe_blending(self, images, labels):\n",
    "        \"\"\"Cr√©e des m√©langes avec protection contre les erreurs\"\"\"\n",
    "        print(\"\\nüé≠ Cr√©ation de Face Blending s√©curis√©...\")\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            print(\"‚ùå Aucune image disponible pour le blending!\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        blended_images = []\n",
    "        blended_labels = []\n",
    "        blend_metadata = []\n",
    "        \n",
    "        # Statistiques pour suivre la g√©n√©ration\n",
    "        stats = {\"total_attempts\": 0, \"successful_blends\": 0, \"errors\": 0}\n",
    "        \n",
    "        for class_id in tqdm(range(self.config.NUM_CLASSES), desc=\"Safe Blending\"):\n",
    "            class_mask = labels == class_id\n",
    "            class_images = images[class_mask]\n",
    "            \n",
    "            if len(class_images) < 2:\n",
    "                print(f\"   ‚ö†Ô∏è  Classe {class_id}: seulement {len(class_images)} image(s), ignor√©e\")\n",
    "                continue\n",
    "            \n",
    "            # G√©n√©rer des m√©langes\n",
    "            for alpha in self.config.BLEND_RATIOS:\n",
    "                for aug_idx in range(self.config.AUGMENTATION_FACTOR):\n",
    "                    stats[\"total_attempts\"] += 1\n",
    "                    \n",
    "                    try:\n",
    "                        # S√©lectionner deux images diff√©rentes\n",
    "                        idx1, idx2 = np.random.choice(len(class_images), 2, replace=False)\n",
    "                        \n",
    "                        img1 = class_images[idx1].astype(np.float32) / 255.0\n",
    "                        img2 = class_images[idx2].astype(np.float32) / 255.0\n",
    "                        \n",
    "                        # M√©lange avec variation l√©g√®re de l'alpha\n",
    "                        alpha_varied = alpha + np.random.normal(0, 0.05)\n",
    "                        alpha_varied = np.clip(alpha_varied, 0.2, 0.8)\n",
    "                        \n",
    "                        blended = alpha_varied * img1 + (1 - alpha_varied) * img2\n",
    "                        \n",
    "                        # Ajouter du bruit l√©ger\n",
    "                        noise = np.random.normal(0, 0.01, blended.shape)\n",
    "                        blended = np.clip(blended + noise, 0, 1)\n",
    "                        \n",
    "                        # Appliquer une l√©g√®re variation de luminosit√©\n",
    "                        brightness_factor = np.random.uniform(0.95, 1.05)\n",
    "                        blended = np.clip(blended * brightness_factor, 0, 1)\n",
    "                        \n",
    "                        # Reconvertir en uint8\n",
    "                        blended_uint8 = (blended * 255).astype(np.uint8)\n",
    "                        \n",
    "                        # V√©rification de qualit√© moins stricte\n",
    "                        if blended_uint8.std() > 1:  # Tr√®s permissif\n",
    "                            blended_images.append(blended_uint8)\n",
    "                            blended_labels.append(class_id)\n",
    "                            \n",
    "                            blend_metadata.append({\n",
    "                                'class_id': class_id,\n",
    "                                'alpha': alpha_varied,\n",
    "                                'source_indices': [idx1, idx2],\n",
    "                                'aug_idx': aug_idx\n",
    "                            })\n",
    "                            \n",
    "                            stats[\"successful_blends\"] += 1\n",
    "                        else:\n",
    "                            stats[\"errors\"] += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        stats[\"errors\"] += 1\n",
    "                        continue\n",
    "        \n",
    "        # Protection contre division par z√©ro\n",
    "        success_rate = (stats[\"successful_blends\"] / stats[\"total_attempts\"]) if stats[\"total_attempts\"] > 0 else 0\n",
    "        \n",
    "        print(f\"‚úÖ Blending termin√©:\")\n",
    "        print(f\"   - Tentatives: {stats['total_attempts']}\")\n",
    "        print(f\"   - R√©ussites: {stats['successful_blends']}\")\n",
    "        print(f\"   - Erreurs: {stats['errors']}\")\n",
    "        print(f\"   - Taux de succ√®s: {success_rate:.2%}\")\n",
    "        \n",
    "        if len(blended_images) == 0:\n",
    "            print(\"‚ö†Ô∏è  Aucun m√©lange g√©n√©r√© avec succ√®s!\")\n",
    "            # Cr√©er des donn√©es factices pour √©viter l'erreur\n",
    "            dummy_img = np.random.randint(0, 255, (*self.config.IMG_SIZE, 3), dtype=np.uint8)\n",
    "            return np.array([dummy_img]), np.array([0]), [{'class_id': 0, 'alpha': 0.5}]\n",
    "        \n",
    "        return np.array(blended_images, dtype=np.uint8), np.array(blended_labels), blend_metadata\n",
    "\n",
    "# Pipeline corrig√©\n",
    "print(\"üöÄ D√©but du chargement corrig√©...\")\n",
    "fixed_loader = FixedDataLoader(config)\n",
    "images, labels = fixed_loader.load_balanced_lfw()\n",
    "blended_images, blended_labels, blend_metadata = fixed_loader.create_safe_blending(images, labels)\n",
    "\n",
    "# V√©rifier que nous avons des donn√©es\n",
    "print(f\"\\nüìä V√©rification des donn√©es:\")\n",
    "print(f\"   - Images originales: {len(images)}\")\n",
    "print(f\"   - Images blend√©es: {len(blended_images)}\")\n",
    "print(f\"   - Labels originaux: {len(labels)}\")\n",
    "print(f\"   - Labels blend√©s: {len(blended_labels)}\")\n",
    "\n",
    "if len(images) > 0 and len(blended_images) > 0:\n",
    "    print(\"‚úÖ Donn√©es pr√™tes pour l'entra√Ænement!\")\n",
    "    \n",
    "    # Continuer avec le pipeline ML\n",
    "    pipeline = ImprovedMLPipeline(config)\n",
    "    data = pipeline.prepare_robust_data(images, labels, blended_images, blended_labels)\n",
    "    \n",
    "    X_train_processed, X_test_processed = pipeline.apply_robust_pca(\n",
    "        data['X_train'], data['X_test']\n",
    "    )\n",
    "    \n",
    "    models = pipeline.train_multiple_models(X_train_processed, data['y_train'])\n",
    "    results = pipeline.comprehensive_evaluation(X_test_processed, data['y_test'])\n",
    "    \n",
    "    print(f\"\\nüéä Pipeline termin√©!\")\n",
    "    if results:\n",
    "        best_acc = max(result['accuracy'] for result in results.values())\n",
    "        print(f\"üèÜ Meilleure accuracy: {best_acc:.2%}\")\n",
    "else:\n",
    "    print(\"‚ùå Pas assez de donn√©es pour l'entra√Ænement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8e6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ LANCEMENT DU PIPELINE COMPLET\n",
      "======================================================================\n",
      "\n",
      "üìä Pr√©paration robuste des donn√©es...\n",
      "   Distribution finale des classes:\n",
      "\n",
      "üìà Statistiques des features:\n",
      "   - Moyenne: nan\n",
      "   - Std: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 223\u001b[0m\n\u001b[0;32m    220\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m ImprovedMLPipeline(config)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# 1. Pr√©paration des donn√©es\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_robust_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblended_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblended_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# 2. Application PCA robuste\u001b[39;00m\n\u001b[0;32m    226\u001b[0m X_train_processed, X_test_processed \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mapply_robust_pca(\n\u001b[0;32m    227\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    228\u001b[0m )\n",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m, in \u001b[0;36mImprovedMLPipeline.prepare_robust_data\u001b[1;34m(self, images, labels, blended_images, blended_labels)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Moyenne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Max: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Division stratifi√©e avec v√©rification\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:48\u001b[0m, in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "class FinalFixedDataLoader:\n",
    "    \"\"\"Version finale corrig√©e - sans filtrage trop restrictif\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.class_info = {}\n",
    "        \n",
    "    def load_robust_lfw(self):\n",
    "        \"\"\"Charge le dataset sans filtrage excessif\"\"\"\n",
    "        print(\"\\nüì• Chargement LFW (Version Finale Corrig√©e)...\")\n",
    "        \n",
    "        # Charger avec param√®tres optimis√©s\n",
    "        lfw_people = fetch_lfw_people(\n",
    "            min_faces_per_person=self.config.MIN_SAMPLES_PER_CLASS,\n",
    "            resize=0.4,\n",
    "            color=False\n",
    "        )\n",
    "        \n",
    "        # Analyser la distribution des classes\n",
    "        unique_labels, counts = np.unique(lfw_people.target, return_counts=True)\n",
    "        print(f\"   Distribution des classes disponibles:\")\n",
    "        for i, (label, count) in enumerate(zip(unique_labels[:10], counts[:10])):\n",
    "            print(f\"      Classe {label}: {count} images\")\n",
    "        \n",
    "        # S√©lectionner les classes avec le plus d'√©chantillons\n",
    "        top_classes = unique_labels[np.argsort(counts)[-self.config.NUM_CLASSES:]]\n",
    "        mask = np.isin(lfw_people.target, top_classes)\n",
    "        \n",
    "        selected_images = lfw_people.images[mask]\n",
    "        selected_labels = lfw_people.target[mask]\n",
    "        \n",
    "        # Remapper les labels\n",
    "        label_mapping = {old: new for new, old in enumerate(top_classes)}\n",
    "        mapped_labels = np.array([label_mapping[label] for label in selected_labels])\n",
    "        \n",
    "        print(f\"\\n‚úÖ Classes s√©lectionn√©es:\")\n",
    "        unique_mapped, mapped_counts = np.unique(mapped_labels, return_counts=True)\n",
    "        for label, count in zip(unique_mapped, mapped_counts):\n",
    "            print(f\"   Classe {label}: {count} images\")\n",
    "            self.class_info[label] = count\n",
    "        \n",
    "        # Preprocessing SIMPLIFI√â (suppression du filtrage probl√©matique)\n",
    "        processed_images = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        print(\"\\nüîß Preprocessing simplifi√©...\")\n",
    "        \n",
    "        # V√©rifier d'abord quelques images pour comprendre la distribution\n",
    "        sample_stds = []\n",
    "        for i in range(min(10, len(selected_images))):\n",
    "            img = selected_images[i]\n",
    "            img_resized = cv2.resize(img, self.config.IMG_SIZE)\n",
    "            sample_stds.append(img_resized.std())\n",
    "        \n",
    "        print(f\"   √âcart-types √©chantillon: {sample_stds}\")\n",
    "        print(f\"   Min std: {min(sample_stds):.2f}, Max std: {max(sample_stds):.2f}, Moyenne: {np.mean(sample_stds):.2f}\")\n",
    "        \n",
    "        # Processus sans filtrage excessif\n",
    "        rejected_stats = {\"processing_error\": 0, \"valid\": 0}\n",
    "        \n",
    "        for i, (img, label) in enumerate(tqdm(zip(selected_images, mapped_labels), \n",
    "                                            total=len(selected_images), desc=\"Processing\")):\n",
    "            try:\n",
    "                # Redimensionner\n",
    "                img_resized = cv2.resize(img, self.config.IMG_SIZE)\n",
    "                \n",
    "                # PAS DE FILTRAGE DE QUALIT√â - accepter toutes les images\n",
    "                # (Le filtrage √©tait trop restrictif pour les images LFW)\n",
    "                \n",
    "                # Convertir en couleur\n",
    "                if len(img_resized.shape) == 2:\n",
    "                    img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                # Normaliser correctement\n",
    "                img_normalized = img_resized.astype(np.uint8)\n",
    "                \n",
    "                processed_images.append(img_normalized)\n",
    "                valid_labels.append(label)\n",
    "                rejected_stats[\"valid\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                rejected_stats[\"processing_error\"] += 1\n",
    "                continue\n",
    "        \n",
    "        final_images = np.array(processed_images, dtype=np.uint8)\n",
    "        final_labels = np.array(valid_labels)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset preprocessed:\")\n",
    "        print(f\"   - Images finales: {len(final_images)}\")\n",
    "        print(f\"   - Images avec erreurs: {rejected_stats['processing_error']}\")\n",
    "        print(f\"   - Images valides: {rejected_stats['valid']}\")\n",
    "        print(f\"   - Classes finales: {len(np.unique(final_labels))}\")\n",
    "        print(f\"   - Forme: {final_images.shape}\")\n",
    "        print(f\"   - M√©moire: ~{final_images.nbytes / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return final_images, final_labels\n",
    "    \n",
    "    def create_guaranteed_blending(self, images, labels):\n",
    "        \"\"\"Cr√©e des m√©langes garantis de fonctionner\"\"\"\n",
    "        print(\"\\nüé≠ Cr√©ation de Face Blending garantie...\")\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            print(\"‚ùå Aucune image disponible pour le blending!\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        blended_images = []\n",
    "        blended_labels = []\n",
    "        blend_metadata = []\n",
    "        \n",
    "        # Statistiques pour suivre la g√©n√©ration\n",
    "        stats = {\"total_attempts\": 0, \"successful_blends\": 0}\n",
    "        \n",
    "        print(f\"   Classes disponibles pour blending:\")\n",
    "        for class_id in range(self.config.NUM_CLASSES):\n",
    "            class_mask = labels == class_id\n",
    "            class_images = images[class_mask]\n",
    "            print(f\"      Classe {class_id}: {len(class_images)} images\")\n",
    "        \n",
    "        for class_id in tqdm(range(self.config.NUM_CLASSES), desc=\"Guaranteed Blending\"):\n",
    "            class_mask = labels == class_id\n",
    "            class_images = images[class_mask]\n",
    "            \n",
    "            if len(class_images) < 2:\n",
    "                print(f\"   ‚ö†Ô∏è  Classe {class_id}: seulement {len(class_images)} image(s), ignor√©e\")\n",
    "                continue\n",
    "            \n",
    "            # G√©n√©rer des m√©langes SANS crit√®res restrictifs\n",
    "            for alpha in self.config.BLEND_RATIOS:\n",
    "                for aug_idx in range(self.config.AUGMENTATION_FACTOR):\n",
    "                    stats[\"total_attempts\"] += 1\n",
    "                    \n",
    "                    try:\n",
    "                        # S√©lectionner deux images diff√©rentes\n",
    "                        idx1, idx2 = np.random.choice(len(class_images), 2, replace=False)\n",
    "                        \n",
    "                        img1 = class_images[idx1].astype(np.float32) / 255.0\n",
    "                        img2 = class_images[idx2].astype(np.float32) / 255.0\n",
    "                        \n",
    "                        # M√©lange simple et robuste\n",
    "                        alpha_varied = alpha + np.random.normal(0, 0.05)\n",
    "                        alpha_varied = np.clip(alpha_varied, 0.2, 0.8)\n",
    "                        \n",
    "                        blended = alpha_varied * img1 + (1 - alpha_varied) * img2\n",
    "                        \n",
    "                        # Ajouter du bruit tr√®s l√©ger\n",
    "                        noise = np.random.normal(0, 0.005, blended.shape)\n",
    "                        blended = np.clip(blended + noise, 0, 1)\n",
    "                        \n",
    "                        # Reconvertir en uint8\n",
    "                        blended_uint8 = (blended * 255).astype(np.uint8)\n",
    "                        \n",
    "                        # AUCUN filtrage de qualit√© - accepter tout\n",
    "                        blended_images.append(blended_uint8)\n",
    "                        blended_labels.append(class_id)\n",
    "                        \n",
    "                        blend_metadata.append({\n",
    "                            'class_id': class_id,\n",
    "                            'alpha': alpha_varied,\n",
    "                            'source_indices': [idx1, idx2],\n",
    "                            'aug_idx': aug_idx\n",
    "                        })\n",
    "                        \n",
    "                        stats[\"successful_blends\"] += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"      Erreur dans classe {class_id}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        success_rate = (stats[\"successful_blends\"] / stats[\"total_attempts\"]) if stats[\"total_attempts\"] > 0 else 0\n",
    "        \n",
    "        print(f\"‚úÖ Blending termin√©:\")\n",
    "        print(f\"   - Tentatives: {stats['total_attempts']}\")\n",
    "        print(f\"   - R√©ussites: {stats['successful_blends']}\")\n",
    "        print(f\"   - Taux de succ√®s: {success_rate:.2%}\")\n",
    "        \n",
    "        return np.array(blended_images, dtype=np.uint8), np.array(blended_labels), blend_metadata\n",
    "\n",
    "# Test avec le nouveau loader\n",
    "print(\"üöÄ Test du nouveau chargeur de donn√©es...\")\n",
    "final_loader = FinalFixedDataLoader(config)\n",
    "images, labels = final_loader.load_robust_lfw()\n",
    "\n",
    "if len(images) > 0:\n",
    "    print(f\"\\nüéâ Succ√®s! {len(images)} images charg√©es\")\n",
    "    \n",
    "    # Cr√©er les m√©langes\n",
    "    blended_images, blended_labels, blend_metadata = final_loader.create_guaranteed_blending(images, labels)\n",
    "    \n",
    "    # V√©rifier les r√©sultats\n",
    "    print(f\"\\nüìä V√©rification finale:\")\n",
    "    print(f\"   - Images originales: {len(images)}\")\n",
    "    print(f\"   - Images blend√©es: {len(blended_images)}\")\n",
    "    print(f\"   - Labels originaux: {len(labels)}\")\n",
    "    print(f\"   - Labels blend√©s: {len(blended_labels)}\")\n",
    "    \n",
    "    if len(images) > 0 and len(blended_images) > 0:\n",
    "        print(\"\\n‚úÖ DONN√âES PR√äTES - Lancement du pipeline ML!\")\n",
    "        \n",
    "        # Lancer le pipeline ML\n",
    "        pipeline = ImprovedMLPipeline(config)\n",
    "        data = pipeline.prepare_robust_data(images, labels, blended_images, blended_labels)\n",
    "        \n",
    "        X_train_processed, X_test_processed = pipeline.apply_robust_pca(\n",
    "            data['X_train'], data['X_test']\n",
    "        )\n",
    "        \n",
    "        models = pipeline.train_multiple_models(X_train_processed, data['y_train'])\n",
    "        results = pipeline.comprehensive_evaluation(X_test_processed, data['y_test'])\n",
    "        \n",
    "        print(f\"\\nüèÜ R√âSULTATS FINAUX:\")\n",
    "        if results:\n",
    "            for model_name, result in results.items():\n",
    "                print(f\"   - {model_name}: {result['accuracy']:.2%}\")\n",
    "            \n",
    "            best_acc = max(result['accuracy'] for result in results.values())\n",
    "            print(f\"\\nü•á Meilleure performance: {best_acc:.2%}\")\n",
    "        else:\n",
    "            print(\"‚ùå Aucun mod√®le n'a pu √™tre entra√Æn√©\")\n",
    "    else:\n",
    "        print(\"‚ùå Probl√®me avec la g√©n√©ration des donn√©es\")\n",
    "else:\n",
    "    print(\"‚ùå √âchec du chargement des donn√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed26d1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Chargement du dataset LFW (Optimis√© RAM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2527/2527 [00:00<00:00, 4415.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé≠ G√©n√©ration de Face Blending...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Blending: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 54.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Blending termin√© : 100 images g√©n√©r√©es.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AdvancedCNNClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39mprepare_training_data()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 3. Entra√Ænement\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mAdvancedCNNClassifier\u001b[49m(config)\n\u001b[0;32m     21\u001b[0m clf\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[0;32m     22\u001b[0m clf\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train, X_val, y_val)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AdvancedCNNClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Ta config\n",
    "class Config:\n",
    "    NUM_CLASSES = 50       # On commence petit pour tester\n",
    "    IMG_SIZE = (128, 128)\n",
    "    BLEND_RATIOS = [0.5]\n",
    "    AUGMENTATION_FACTOR = 2\n",
    "    LEARNING_RATE = 1e-3\n",
    "    BATCH_SIZE = 16        # Petit batch pour √©viter le OOM\n",
    "    EPOCHS = 10\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# 2. Pipeline\n",
    "gen = FaceBlendingDataGenerator(config)\n",
    "gen.load_lfw_subset()\n",
    "gen.create_blended_faces()\n",
    "X_train, X_val, y_train, y_val = gen.prepare_training_data()\n",
    "\n",
    "# 3. Entra√Ænement\n",
    "clf = AdvancedCNNClassifier(config)\n",
    "clf.build_model()\n",
    "clf.train(X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
